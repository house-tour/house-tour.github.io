<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HouseTour: A Virtual Real Estate A(I)gent">
  <meta name="keywords" content="HouseTour, Real Estate Tours, 3D Camera Trajectory">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HouseTour: A Virtual Real Estate A(I)gent</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/real-estate.svg">

    <script src="./static/js/jquery-3.2.1.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/jquery.event.move.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HouseTour: A Virtual Real Estate A(I)gent</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://inf.ethz.ch/people/people-atoz/person-detail.MjUxMjk3.TGlzdC8zMDQsLTIxNDE4MTU0NjA=.html">Ata Çelen</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a><sup>1,3</sup>
            </span>
            <span class="author-block">
              <a href="https://cvg.ethz.ch/team/Dr-Daniel-Bela-Barath">Dániel Baráth</a><sup>1,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://ir0.github.io/">Iro Armeni</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ETH Zürich</span><sup>1</sup>,
            <span class="author-block">Stanford University</span><sup>2</sup>
            <span class="author-block">Microsoft Spatial AI Lab</span><sup>3</sup>
            <span class="author-block">HUN-REN SZTAKI</span><sup>4</sup>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a title="To be Published..." 
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a title="To be Published..." 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a title="To be Published..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/GradientSpaces/HouseTour"
                   title="HouseTour repository"
                   class="external-link button is-normal is-rounded is-primary">
                  <span class="icon">
                  <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a title="To be Published..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                  <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="evaluation" width="100%" src="./static/images/teaser.png">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              We introduce <span class="methodname">HouseTour</span>, a method for spatially-aware 3D camera trajectory 
              and natural language summary generation from a collection of images depicting an existing 3D space. 
              Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach 
              generates smooth video trajectories via a diffusion process constrained by known camera poses and 
              integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video 
              using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we 
              present the <span class="methodname">HouseTour</span> dataset, which includes over 1,200 house-tour videos with camera poses, 
              3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories 
              into the text generation process improves performance over methods handling each task independently. 
              We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, 
              professional-quality video creation for real estate and touristic applications without requiring 
              specialized expertise or equipment.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section pt-0">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="container is-max-desktop">
            <div class="hero-body">
              <img id="teaser" width="100%" src="./static/images/architecture.png" alt="Qwen2-VL-3D Architecture">
            </div>
            <div class="content has-text-justified">
                <p>
                  <span class="methodname">HouseTour</span> generates 3D camera trajectories and natural language summaries from RGB images with known poses of real estate spaces.
                  Our approach consists of two primary modules: <b>Residual Diffuser</b> and <b>Qwen2-VL-3D</b>.
                </p>
                <p>
                  Built upon "Diffuser", the <b>Residual Diffuser</b> is a diffusion model that generates 3D camera trajectories, which mirror human-like movement in real estate tours.
                  It utilizes a masked diffusion process to create smooth video trajectories, constrained by known camera poses, and integrates this information into Qwen2-VL-3D for 3D-grounded descriptions.
                  This residual-based approach enables the model to focus on learning the nuanced deviations from the spline baseline that characterize realistic human camera movement, rather than memorizing absolute trajectories for each unique scene. 
                  By predicting only the residuals, the model generalizes better across diverse environments and adapts to the dynamic nature of real estate layouts. 
                  The combination of spline interpolation and learned residuals ensures smooth, plausible trajectories that respect both geometric constraints and human-like navigation patterns. 
                  This formulation also simplifies the conditioning process, as the model only needs to enforce zero residuals at known camera poses, allowing for efficient and deterministic integration of sparse observations. 
                  Overall, our Residual Diffuser provides a flexible and robust solution for trajectory planning in complex, variable 3D spaces.
                </p>
                <p>
                  The <b>Qwen2-VL-3D</b> module is a vision-language model that generates natural language summaries of the 3D space, leveraging the generated camera trajectory.
                  This integration of spatial features enables Qwen2-VL-3D to attend not only to the visual content of each frame but also to its precise location and movement within the property. 
                  By encoding both the pose and the learned trajectory features, the model can incorporate information about the spatial relationships between rooms, the flow of movement, and the overall layout, resulting in more contextually accurate and informative summaries. 
                  During inference, the model leverages these spatial tokens to generate descriptions that reference architectural highlights, closely emulating the narrative style of professional house tours. 
                  This approach ensures that the generated summaries are grounded in both the visual and spatial realities of the property, providing users with coherent, detailed, and spatially-aware walkthroughs.
                </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<section>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">HouseTour Dataset</h2>
          <div class="hero-body">
            <img id="dataset-gif" width="100%" src="./static/videos/final_grid_900.gif" alt="HouseTour dataset preview GIF">
          </div>
          <div class="content has-text-justified">
            <p>
                The HouseTour Dataset contains over 1,600 real-estate tour videos with aligned camera poses, 3D reconstructions, and professionally written property descriptions, enabling research in multi-modal scene understanding.
                The data is procured from professional real-estate agencies and we acquired rights for sharing and using. Our dataset comprises:
            </p>
            <ul>
              <li><strong>1,639 videos of real estate properties</strong> (apartments, condos, multi-storey houses)</li>
              <li><strong>1,298 transcribed videos</strong> with real-estate oriented textual descriptions</li>
              <li><strong>878 reconstructed 3D scenes</strong> (dense point clouds + camera trajectories)</li>
              <li><strong>Professionally captured smooth camera trajectories</strong> emulating human navigation</li>
              <li><strong>Real-estate style descriptions</strong> emphasizing layout, architectural features, materials, and ambiance</li>
              <li><strong>Privacy-filtered:</strong> personally identifiable or sensitive content removed</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section pt-0">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Evaluation</h2>
          <div class="content has-text-justified">
            <div class="hero-body">
              <img id="evaluation" width="100%" src="./static/images/summary_qualitative_1.png" alt="Scene-Level Summary Generation">
            </div>
            <p>
              Here we present a qualitative example of our summary generation, illustrating how the produced text corresponds to specific spatial elements and sampled images from the environment.
            </p>
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section pt-0">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Acknowledgements</h2>
          <div class="content has-text-justified">
            <p>
              This work was supported by an ETH Zurich Career Seed Award.
            </p>
          </div>
        </div>
      </div>
    </div>

    <br><br>
    <br><br>

<footer class="footer pt-4 pb-0">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        Website template based on
                        <a href="https://github.com/nerfies/nerfies.github.io">
                            Nerfies</a> and
                        <a href="https://marigoldmonodepth.github.io/">
                            Marigold</a>,
                        and licensed under
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
                            CC-BY-SA-4.0</a>.<br>
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>
  
</body>
</html>
